{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook focuses on some basics of data analysis: linear regression, optimization and Bayesian inference.\n"
      ],
      "metadata": {
        "id": "fQEINBQMh2qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least squares linear regression: basics\n",
        "\n"
      ],
      "metadata": {
        "id": "skN3xJRVg7Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by implementing a simple least squares fit using analytical expressions.\n",
        "\n",
        "Plot the data provided below. Calculate the least squares estimate for the slope $a$ and intercept $b$ ($y=ax+b$) with the analytical formulae mentioned in lectures and add the corresponding line to your plot."
      ],
      "metadata": {
        "id": "0cCeJ45hhRBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXRJ3b5qD-Dj"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pylab\n",
        "\n",
        "xarray = numpy.arange(20.)/5\n",
        "yarray = numpy.array([1.16100793, 2.27070892+8.*0., 1.7102285 , 1.92216664 ,2.00306275 ,3.60437835,\n",
        " 3.8682993 , 5.51139028, 3.16760897, 4.27715062, 4.22776111, 5.33520844,\n",
        " 6.50611469, 5.4146256,  7.04786637, 8.44454962, 7.63848438, 7.81296952,\n",
        " 8.80991074 ,6.62531274])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now recompute the least-squares slope $a$ and intercept $b$ by numerically minimizing/optimizing the least squares loss function $\\sum_i (y(x_i)-f(x_i,a,b))^2$. You may wish to use the numerical optimization routine included below. Do you find the same result as in the analytical calculation? Replot the data along with the best fit line you have derived.\n",
        "\n",
        "Repeat your optimization but with an L1 loss function and add the corresponding line to your plot.\n",
        "\n"
      ],
      "metadata": {
        "id": "vD3U4x7XhWlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(coeff,xarray,yarray):\n",
        "  #add your loss function here\n",
        "  return loss\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "start = [1.,1.]\n",
        "\n",
        "minimum = minimize(loss,x0=start,args=(xarray,yarray),method='BFGS')\n",
        "#first parameter minimum.x[0], second minimum.x[1] etc."
      ],
      "metadata": {
        "id": "oMMv4LkSFEMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, perturb one of the data points so that it is a noticeable outlier, and show that the line derived from L1 optimization is less affected than L2."
      ],
      "metadata": {
        "id": "BthRZOzCs8TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding more parameters"
      ],
      "metadata": {
        "id": "e__mMbP0hNH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now consider a new data vector, provided below, which contains an oscillatory feature with a known period and phase but an unknown amplitude $c$. Plot the new data. Using L2/least squares optimization and assuming the model function $f=ax+b+c \\sin(5x)$, find the best fit three parameter model (described by the parameters $a, b, c$.)"
      ],
      "metadata": {
        "id": "fXB7JZc8vT5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new data vector with oscillatory feature\n",
        "xarray = numpy.arange(20.)/5\n",
        "yarray = numpy.array([1.16100793, 1.42923794, 0.80093107, 1.78104663, 2.75986525, 4.56330262,\n",
        " 4.1477148,  4.85440368, 2.17825072, 3.86503213, 4.77178222, 6.33519865,\n",
        " 7.04268761, 4.99445856, 6.05725901, 7.79426178, 7.9263877,  8.77436701,\n",
        " 9.56089799, 6.47543553])\n",
        ""
      ],
      "metadata": {
        "id": "D4GhCkF9WNNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Bayesian Data Analysis"
      ],
      "metadata": {
        "id": "EgxBoSXWhEf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now turn to Bayesian data analysis. Our original data vector (without oscillations) now has errors, provided below. Write down a likelihood, a prior (you may assume it is just a constant for now), and evaluate the posterior probability on a grid of slope $a$ and intercept $b$ values. Plot this 2D posterior."
      ],
      "metadata": {
        "id": "6e6F02-K1Onp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The original data vector, but this time with errors!\n",
        "xarray = numpy.arange(20.)/5\n",
        "yarray = numpy.array([1.16100793, 2.27070892, 1.7102285 , 1.92216664 ,2.00306275 ,3.60437835,\n",
        " 3.8682993 , 5.51139028, 3.16760897, 4.27715062, 4.22776111, 5.33520844,\n",
        " 6.50611469, 5.4146256,  7.04786637, 8.44454962, 7.63848438, 7.81296952,\n",
        " 8.80991074 ,6.62531274])\n",
        "yarrayError = yarray*0.+1.\n",
        "\n",
        "\n",
        "#### you may wish to use this function\n",
        "def evaluate_on_grid(funct, a_grid, b_grid):\n",
        "#### returns 2d array of function values evaluated on a, b grid values\n",
        "    a_grid, b_grid = numpy.meshgrid(a_grid, b_grid)\n",
        "    abgrid = numpy.vstack((a_grid.ravel(), b_grid.ravel())).T\n",
        "    vals = numpy.zeros(abgrid.shape[0])\n",
        "    for j, pars in enumerate(abgrid):\n",
        "        vals[j] = funct(pars)\n",
        "    return vals.reshape(a_grid.shape)\n",
        "\n",
        "#### example usage with a function loglikelihood (function of a 2D array)\n",
        "a_grid = numpy.linspace(-3.,7.,256)\n",
        "b_grid = numpy.linspace(-4.,6.,256)\n",
        "logongrid = evaluate_on_grid(loglikelihood,a_grid,b_grid)\n"
      ],
      "metadata": {
        "id": "THWNcfOnRHlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, derive the marginalized posterior probability for $a$ by integrating or summing the posterior over the $b$ axis. Plot your result. Repeat this for the posterior on $b$."
      ],
      "metadata": {
        "id": "n5i709Rx6WSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#p_a_marg ="
      ],
      "metadata": {
        "id": "EqEChGzp3_-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the above Bayesian parameter analysis for the sinusoidal dataset and 3-parameter fit. Derive marginalized posteriors for $a$, $b$, and $c$.\n"
      ],
      "metadata": {
        "id": "aaKGhyqw8GHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loglikelihood2(vals):\n",
        "  loglike2 = #### Write down log(likelihood) function\n",
        "  return loglike2\n",
        "\n",
        "#### you may wish to use this function - a 3d version of evaluate_on_grid\n",
        "def evaluate_on_grid2(funct, a_grid, b_grid,c_grid):\n",
        "    #### returns 3d array of funct values evaluated on a, b, c grid values\n",
        "    a_grid, b_grid, c_grid = numpy.meshgrid(a_grid, b_grid, c_grid)\n",
        "    abc_grid = numpy.vstack((a_grid.ravel(), b_grid.ravel(), c_grid.ravel())).T\n",
        "    vals = numpy.zeros(abc_grid.shape[0])\n",
        "    for j, pars in enumerate(abc_grid):\n",
        "        vals[j] = funct(pars)\n",
        "    return vals.reshape(a_grid.shape)\n",
        "\n",
        "#use data vector with oscillatory feature\n",
        "xarray = numpy.arange(20.)/5\n",
        "yarray = numpy.array([1.16100793, 1.42923794, 0.80093107, 1.78104663, 2.75986525, 4.56330262,\n",
        " 4.1477148,  4.85440368, 2.17825072, 3.86503213, 4.77178222, 6.33519865,\n",
        " 7.04268761, 4.99445856, 6.05725901, 7.79426178, 7.9263877,  8.77436701,\n",
        " 9.56089799, 6.47543553])\n",
        "\n",
        "\n",
        "#see below for example calculation of marginalized posterior for the slope parameter a.\n",
        "#this evaluation uses only 60 grid points in each dimension\n",
        "a_grid = numpy.linspace(0.,4.,60)\n",
        "b_grid = numpy.linspace(-1.,3.,60)\n",
        "c_grid = numpy.linspace(-3.,1.,60)\n",
        "\n",
        "logongrid2 = evaluate_on_grid2(loglikelihood2,a_grid,b_grid,c_grid)\n",
        "ongrid2 = numpy.exp(logongrid2)\n",
        "# note: some care is required to integrate over the correct axes\n",
        "p_a_marg = numpy.sum(ongrid2,axis=(0,2))\n"
      ],
      "metadata": {
        "id": "Nuh_fZhE9svV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the evaluation gets very slow when we make our grid finer! I.e. if we change numpy.linspace(...,60) to a significantly larger number of grid points the algorithm cannot be run. A faster method, which scales well to larger number of parameters, is required... MCMC."
      ],
      "metadata": {
        "id": "lcXZR_Nk_-UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCMC\n"
      ],
      "metadata": {
        "id": "OyzL_NtiwG5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will solve the same problem with a Markov Chain Monte Carlo method. If you have lots of time left, write your own Metropolis Hastings algorithm. Otherwise, examine and add comments explaining the code written below."
      ],
      "metadata": {
        "id": "t7bGUaP_Gksx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MCMC(parameters,stepnumber,logPosteriorFn,sigmas):\n",
        "  chain = numpy.zeros((stepnumber,len(parameters)))\n",
        "  logProb = numpy.zeros(stepnumber)\n",
        "  chain[0] = parameters\n",
        "  logProb[0] = logPosteriorFn(parameters)\n",
        "  for i in range(1,stepnumber):\n",
        "    diff = numpy.random.normal(sigmas)\n",
        "    newParam = chain[i-1]+diff\n",
        "    newValue = logPosteriorFn(newParam)\n",
        "    logProbRatio = newValue - logPosteriorFn(chain[i-1])\n",
        "    if (logProbRatio > 0) or (numpy.exp(logProbRatio) >  numpy.random.uniform()):\n",
        "      chain[i] = newParam\n",
        "      logProb[i] = newValue\n",
        "    else:\n",
        "      chain[i] = chain[i-1]\n",
        "      logProb[i] = logProb[i-1]\n",
        "  return chain, logProb\n"
      ],
      "metadata": {
        "id": "keDDWiKtwQb_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run a chain as shown below. Plot the values of $a$ vs. the values of $b$ for the first 3000 steps in the chain and explain this plot. Why are the initial steps often excluded?\n",
        "\n",
        "Generate and plot a histogram of the $a$ values in the chain. Comment on the significance / interpretation of this plot.\n",
        "\n"
      ],
      "metadata": {
        "id": "N6CjbXODPDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### can use this along with loglikelihood function to run chains\n",
        "parameters = numpy.array([1.,1.,1.])\n",
        "stepnumber = 100000\n",
        "sigmas = numpy.array([0.1,0.1,0.1])\n",
        "chain, logProb = MCMC(parameters,stepnumber,loglikelihood2,sigmas)\n",
        "#### note: chain is chain[stepnumber,parameter_index]"
      ],
      "metadata": {
        "id": "Zb-AAh3vCThO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the corner package to plot the posteriors. Explain what each panel of the plot is showing."
      ],
      "metadata": {
        "id": "vpsatmURTxxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!source activate statsseminar; pip install corner\n",
        "import corner\n",
        "fig = corner.corner(\n",
        "    chain, bins=15, labels=[\"$a$\", \"$b$\",\"$c$\"]\n",
        ")"
      ],
      "metadata": {
        "id": "fCV0LubEm0ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining the impact of priors"
      ],
      "metadata": {
        "id": "Q_cNASOHjQ8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now examine the impact of priors. Set a prior that imposes a lower bound on one of the parameters.  Then impose a prior such that $P(c) \\propto e^{10 c}$. Examine the resulting posterior plots for each case."
      ],
      "metadata": {
        "id": "h3Qnw1JPVLhF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPI7SAU1tKdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: some aspects of this notebook are inspired by Hogg et al. https://arxiv.org/abs/1008.4686 or a [blogpost](https://adrian.pw/blog/fitting-a-line/) by Adrian Price-Whelan – these are also useful references for students seeking more details on some of the material."
      ],
      "metadata": {
        "id": "F2xd4AG0hzpy"
      }
    }
  ]
}